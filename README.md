# Joint Modeling of Source, Target, and Context for Metaphor Sentiment Analysis: Dataset and Method

<!-- **Authors:** Anonymous Authors -->


<!-- [[è®ºæ–‡PDF]()] [[arXiv]()] [[é¡¹ç›®ä¸»é¡µ]()] -->

<!-- ![Teaser Figure](docs/teaser.png) -->

>This repository contains the official implementation and experimental resources for the paper **[Joint Modeling of Source, Target, and Context for Metaphor Sentiment Analysis: Dataset and Method]**

## ğŸ“ Abstract
Metaphorical expressions evoke stronger emotions than literal language. However, their implicit sentiment mapping characteristics pose significant challenges for sentiment analysis. Existing studies exhibit two major limitations: first, most models focus on modeling the partial of three core elements (the source-domain word $S$, the target-domain word $T$, and the metaphorical context $C$), neglecting the interactions among them in metaphors; second, current research in computational metaphor predominantly adopts a single-type metaphor pattern, making it difficult for models to capture shared representations across different metaphor types, thereby limiting their generalization. To address these issues, this study proposes a multi-type metaphor sentiment analysis framework that jointly models $S$, $T$, and $C$, aiming to accurately identify the sentiment of complex metaphorical texts while revealing the path of metaphorical sentiment transfer, thereby facilitating sentiment analysis of metaphor-rich texts. To effectively model the three elements, a metaphor sentiment analysis model integrating Multi-Task Contrastive Learning and Instruction Tuning (MTCL-IT) is proposed. Finally, multi-perspective experiments are conducted on the constructed English dataset EMSA and Chinese dataset CMSA, both containing $S$, $T$, sentiment polarity, and metaphor type information, fully validating the effectiveness of the proposed framework and model in sentiment recognition and understanding of sentiment mapping mechanisms.



## ğŸ—‚ Repository Structure
```angular2html
â”œâ”€â”€ ğŸ“ checkpoints/          # Save the enhanced pre-trained Metaphor-BERT
â”œâ”€â”€ ğŸ“ data/                 # Datasets
â”œâ”€â”€ ğŸ“ docs/                 # Documentation & figures
â””â”€â”€ ğŸ“ src/                  # Core implementations
    â”œâ”€â”€ ğŸ“ instruct/         # KNN-based intruction construction
    â”œâ”€â”€ ğŸ“ models/           # Core models
    â”œâ”€â”€ ğŸ“ utils/            # Data process & evaluation & some utils
    â”œâ”€â”€ ğŸ“„ main.py           # Main excution script
    â””â”€â”€ ğŸ“„ pretrain.py       # Enhanced pre-trained for BERT yields Metaphor-BERT
```

## ğŸ›  Installation

### Requirements
- Python â‰¥ 3.8
- PyTorch â‰¥ 2.0.0
- CUDA â‰¥ 11.8

### Setup
```bash
# Clone repository
git clone https://github.com/XXXX/xxxxx.git
cd MTCL-IT

# Create conda environment
conda create -n msa python=3.8
conda activate msa

# Install dependencies
pip install -r requirements.txt
```
## ğŸš€ Usage
- **Enhanced Pre-training**
```bash
# Please modify the data and model paths
python pretrain.py
```

- **Training & Testing**

```bash
CUDA_VISIBLE_DEVICES=0 bash run.sh
```

<!-- ## ğŸ“Š Results -->
<!-- Performance on [Dataset Name]:

Method	Metric1	Metric2	Metric3
Our Method	0.92	1.23	95.4%
Baseline	0.85	1.45	89.7%
Results Comparison -->

<!-- ## ğŸ“œ Citation -->
<!-- ```bash
@article{yourcitationkey,
  title     = {Your Paper Title},
  author    = {Author1 and Author2 and Author3},
  journal   = {Conference or Journal Name},
  year      = {2023}
}
``` -->


<!-- ## ğŸ“§ è”ç³»æ–¹å¼ -->
<!-- Corresponding Author: [Your Name] - your.email@institution.edu -->
